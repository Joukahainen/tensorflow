{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Close Price Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ml_analysis import MLOperator, MLEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tf'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-11-3a1467ac3556>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32massert\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__version__\u001B[0m \u001B[0;34m>=\u001B[0m \u001B[0;34m\"2.0\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0msklearn\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'tf'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "import tf.keras\n",
    "\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Pandas Dataframe, viewing the first ten rows and the distribution of the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/HFT.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.label.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that there is a class imbalance problem. Proceeding without correcting for the imbalance will lead to spurious results. \n",
    "\n",
    "Our exposition includes examples of exploratory data analysis, designed to provide intuition into the level of non-linearity in the map $Y=F(X)$. Let's first view the distribution of our features by each label 0,-1, then 1. \n",
    "\n",
    "*Note: We just consider feature_1, but you should check the other features too.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for lab in df.label.unique():\n",
    "    print(lab)\n",
    "    df.feature_1[df.label==lab].hist()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Preparation of the training and testing sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a simple 80/20 splitting rule, ensuring that the training set pre-dates the testing set (to avoid look-ahead bias)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_weight = 0.8\n",
    "split = int(len(df)*train_weight)\n",
    "df_train = df[:split]\n",
    "df_test = df[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate our MLOperator class which will be used for performing high-level data processing tasks on top of Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "operator = MLOperator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "\n",
    "**Training Set**: Resolve the time series class imbalance problem by sampling in a neighborhood of the up and down ticks. We choose this neighborhood to be a lagging window of size 20. Although simplistic, this preserves at least some of the covariance structure in the time series data.\n",
    "\n",
    "**Testing Set**: Do not sample - leave imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_steps = 20\n",
    "use_features = ['feature_1']\n",
    "y_train = df_train.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampled_idx = operator.get_samples_index(y_train.iloc[n_steps-1:], 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = df_train[use_features]\n",
    "y_test = df_test.label.iloc[n_steps-1:]\n",
    "x_test = df_test[use_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data formatting for LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the following function for reshaping the data into times series classification format. For example, consider a univariate time series of increasing integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the sequence length to 10, we move the window forward by one observation at a time and construct new sequences:\n",
    "\n",
    "1 2 3 4 5 6 7 8 9 10\n",
    "\n",
    "2 3 4 5 6 7 8 9 10 11\n",
    "\n",
    "3 4 5 6 7 8 9 10 11 12\n",
    "\n",
    "4 5 6 7 8 9 10 11 12 13\n",
    "\n",
    "5 6 7 8 9 10 11 12 13 14\n",
    "\n",
    "6 7 8 9 10 11 12 13 14 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_LSTM_data(x_train, y_train, x_test, y_test, sampled_idx, n_steps):\n",
    "    # training set\n",
    "    x_train_m = x_train.values()\n",
    "    x_train_list = []\n",
    "    for idx in sampled_idx:\n",
    "        int_idx = y_train.index.get_loc(idx)\n",
    "        x_train_list.append(x_train_m[(int_idx-n_steps+1):int_idx+1])\n",
    "\n",
    "    x_train = np.array(x_train_list)\n",
    "    y_train = pd.get_dummies(y_train[sampled_idx]).values()\n",
    "\n",
    "    # test set\n",
    "    x_test_m = x_test.values()\n",
    "    x_test_list = []\n",
    "    for i in range(x_test.shape[0] - n_steps+1):\n",
    "        x_test_list.append(x_test_m[i: (i + n_steps)])\n",
    "    x_test = np.array(x_test_list)\n",
    "\n",
    "    y_test = pd.get_dummies(y_test).values()\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = get_LSTM_data(x_train, y_train, x_test, y_test, sampled_idx, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nrow = 3000\n",
    "x_valid = x_test[0:nrow]\n",
    "y_valid = y_test[0:nrow]\n",
    "x_test = x_test[nrow:]\n",
    "y_test = y_test[nrow:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(x_train.shape,y_train.shape,x_test.shape,y_test.shape, x_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model Specification\n",
    "\n",
    "We now proceed with the model specification in Keras. Consistent with the window size, we specify a ten layer LSTM network and squash each unit output with a softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(10, stateful=False, input_shape=(x_train.shape[1], x_train.shape[-1])))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configurations\n",
    "initial_lrate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adagrad = optimizers.Adagrad(lr=initial_lrate, epsilon=1e-08, decay=0.0)\n",
    "adam = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callbacks_list = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# learning rate schedule\n",
    "# LearningRate = InitialLearningRate * DropRate^floor(Epoch / EpochDrop)\n",
    "# Or: decrease half of the learning rate after every epochs_drop (20 epochs for example)\n",
    "def step_decay(epoch):\n",
    "    lrate = initial_lrate\n",
    "    epochs_drop = 70\n",
    "    drop = 0.5\n",
    "    if (epoch % epochs_drop == 0):\n",
    "        lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "        print(\"Decreased Learning Rate to: {}\".format(lrate))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrate = LearningRateScheduler(step_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callbacks_list.append(lrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initial settings\n",
    "n_epoch = 20\n",
    "n_batch = 500\n",
    "last_epoch_th = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit(x_train, y_train, epochs=n_epoch, batch_size=n_batch, verbose=1,\n",
    "          validation_data=(x_valid, y_valid), shuffle=True, callbacks = callbacks_list, \n",
    "                    initial_epoch = last_epoch_th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialization\n",
    "If no further adjustments to the training are needed then we can save the model to HDF5 format and simply reload it in future notebook sessions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save('model/lstm.hdf5', overwrite=True)  # creates a HDF5 file 'my_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history_path = \"history/lstm_history.json\"\n",
    "with open(history_path, 'w') as fp:\n",
    "    json.dump(hist, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model = load_model('model/lstm.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "pred_df_train, pred_df_test, _ = operator.get_pred_dfs(x_train, x_test, y_train, y_test, \n",
    "                                                       classifier='tensorflow', model=model, \n",
    "                                                       columns=['-1', '0', '1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_f1 = f1_score(pred_df_train.true, pred_df_train.predict, average=None)\n",
    "test_f1 = f1_score(pred_df_test.true, pred_df_test.predict, average=None)\n",
    "print(train_f1)\n",
    "print(test_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate our MLEvaluator class to store the performance results of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluator = MLEvaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, our primary objective in assessing our LSTM model, is to evaluate the extent of over-fitting. One simple method for measuring the bias-variance tradeoff compares the performance of the model on the training and test set. We use confusion matrices and ROC curves to measure the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluator.set_pred_df(pred_df_train)\n",
    "cf_mx_train = evaluator.generate_confusion_matrix()\n",
    "\n",
    "fig = evaluator.plot_confusion_matrix(cf_mx_train.values(), [-1, 0, 1])\n",
    "fig = evaluator.plot_confusion_matrix(cf_mx_train.values(), [-1, 0, 1], \n",
    "    normalize=True)\n",
    "fig = evaluator.plot_roc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluator.set_pred_df(pred_df_test)\n",
    "cf_mx_test = evaluator.generate_confusion_matrix()\n",
    "\n",
    "fig = evaluator.plot_confusion_matrix(cf_mx_test.values(), [-1, 0, 1])\n",
    "fig = evaluator.plot_confusion_matrix(cf_mx_test.values(), [-1, 0, 1], \n",
    "    normalize=True)\n",
    "fig = evaluator.plot_roc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  Learning Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seek to evaluate the bias-variance tradeoff as a function of the size of the training set. This can address the question of whether the sample size is 'sufficient' for our classifier - we expect the performance on the test set to converge to that on the training set with increasing sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_bvt = df_train.label\n",
    "x_train_bvt = df_train[use_features]\n",
    "y_test_bvt = df_test.label.iloc[n_steps-1:]\n",
    "x_test_bvt = df_test[use_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_exp = 1\n",
    "n_obs_list = range(100, len(sampled_idx)/3, 300)\n",
    "print(n_obs_list)\n",
    "\n",
    "# initial settings\n",
    "n_epoch = 20\n",
    "n_batch = 500\n",
    "last_epoch_th = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1_df_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lrate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-12-f3e48290c4a1>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mbvt_callbacks_list\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mbvt_callbacks_list\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlrate\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'lrate' is not defined"
     ]
    }
   ],
   "source": [
    "bvt_callbacks_list = list()\n",
    "bvt_callbacks_list.append(lrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for exp in range(n_exp):\n",
    "    print('experiment ', exp)\n",
    "    for n_obs in n_obs_list:\n",
    "        print('number of observations ', n_obs)\n",
    "        sampled_idx_sm = operator.get_samples_index(y_train_bvt.iloc[n_steps-1:], n_obs)\n",
    "        print(len(sampled_idx_sm))\n",
    "        \n",
    "        x_train_sm, y_train_sm, x_test_sm, y_test_sm = get_LSTM_data(x_train_bvt, y_train_bvt, x_test_bvt, y_test_bvt, \n",
    "                                                                     sampled_idx_sm, n_steps)\n",
    "        \n",
    "        print(x_train_sm.shape,y_train_sm.shape,x_test_sm.shape,y_test_sm.shape)\n",
    "\n",
    "        model.fit(x_train_sm, y_train_sm, epochs=n_epoch, batch_size=n_batch, verbose=2,\n",
    "                    shuffle=True, callbacks = bvt_callbacks_list, \n",
    "                    initial_epoch = last_epoch_th)\n",
    "        \n",
    "        pred_df_train, pred_df_test, _ = operator.get_pred_dfs(x_train_sm, x_test_sm, y_train_sm, y_test_sm, \n",
    "                                                       classifier='tensorflow', model=model, \n",
    "                                                       columns=['-1', '0', '1'])\n",
    "        \n",
    "        train_f1 = f1_score(pred_df_train.true, pred_df_train.predict, average=None)\n",
    "        test_f1 = f1_score(pred_df_test.true, pred_df_test.predict, average=None)\n",
    "\n",
    "        f1_df = pd.DataFrame([train_f1, test_f1])\n",
    "        f1_df.columns = ['-1', '0', '1']\n",
    "        f1_df.index = ['train', 'test']\n",
    "        f1_df['n_obs'] = n_obs * 3\n",
    "        f1_df_list.append(f1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_f1_df = pd.concat(f1_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_f1_df.to_csv('history/lstm_bias_variance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_f1_df = pd.read_csv('history/lstm_bias_variance.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_f1_df['macro_avg'] = final_f1_df[['-1','0','1']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for label in ['macro_avg', '1', '0', '-1']:\n",
    "    evaluator.plot_learning_curve(final_f1_df, label, 'n_obs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Loss function convergence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history_path = \"history/lstm_history.json\"\n",
    "with open(history_path, 'r') as fp:\n",
    "    hist = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 4.5))\n",
    "train_line = plt.plot(range(len(hist['acc'])), hist[\"acc\"],  color=\"b\", label=\"Training\")\n",
    "vald_line = plt.plot(range(len(hist['acc'])), hist[\"val_acc\"], color=\"g\", label=\"Validattion\")\n",
    "plt.legend(loc=\"best\", fontsize=15)\n",
    "plt.ylim(-0.1, 1)\n",
    "plt.title(\"accuracy vs epoch\")\n",
    "plt.xlabel('epoch', fontsize=20)\n",
    "plt.ylabel('score', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 4.5))\n",
    "train_line = plt.plot(range(len(hist['loss'])), hist[\"loss\"],  color=\"b\", label=\"Training\")\n",
    "vald_line = plt.plot(range(len(hist['loss'])), hist[\"val_loss\"], color=\"g\", label=\"Validattion\")\n",
    "plt.legend(loc=\"best\", fontsize=15)\n",
    "plt.title(\"loss vs epoch\")\n",
    "plt.xlabel('epoch', fontsize=20)\n",
    "plt.ylabel('loss', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model Regression\n",
    "\n",
    "An alternative approach to classification is to formulate the response as a continuous variable and predict the 'smart price' as a uni-variate time series. The smart price is the volume weighted mid-price, represented by feature_3 in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_features = ['feature_3']\n",
    "target = 'feature_3'\n",
    "n_steps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_weight = 0.8\n",
    "split = int(len(df)*train_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = df.iloc[:split]\n",
    "std_df_train = df_train[use_features].apply(lambda x: (x - x.mean()) / x.std())\n",
    "df_test = df.iloc[split:]\n",
    "std_df_test = df[use_features].apply(lambda x: (x - x.mean()) / x.std()).iloc[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lagged_features(value, n_steps):\n",
    "    lag_list = []\n",
    "    for lag in range(n_steps, 0, -1):\n",
    "        lag_list.append(value.shift(lag))\n",
    "    return pd.concat(lag_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_list = []\n",
    "for use_feature in use_features:\n",
    "    x_train_reg = get_lagged_features(std_df_train, n_steps).dropna()\n",
    "    x_train_list.append(x_train_reg)\n",
    "x_train_reg = pd.concat(x_train_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_ords = []\n",
    "for i in range(n_steps):\n",
    "    for j in range(len(use_features)):\n",
    "        col_ords.append(i + j * n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train_reg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-13-7616dd1d25f5>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mx_train_reg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mx_train_reg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0miloc\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcol_ords\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0my_train_reg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf_train\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mx_train_reg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mtarget\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mx_train_reg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx_train_reg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mx_train_reg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx_train_reg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0muse_features\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0muse_features\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'x_train_reg' is not defined"
     ]
    }
   ],
   "source": [
    "x_train_reg = x_train_reg.iloc[:, col_ords]\n",
    "y_train_reg = df_train.loc[x_train_reg.index, [target]].values\n",
    "x_train_reg = np.reshape(x_train_reg.values, (x_train_reg.shape[0], x_train_reg.shape[1] / len(use_features), len(use_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test_list = []\n",
    "for use_feature in use_features:\n",
    "    x_test_reg = get_lagged_features(std_df_test, n_steps).dropna()\n",
    "    x_test_list.append(x_test_reg)\n",
    "x_test_reg = pd.concat(x_test_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test_reg = x_test_reg.iloc[:, col_ords]\n",
    "y_test_reg = df_test.loc[x_test_reg.index, [target]].values()\n",
    "x_test_reg = np.reshape(x_test_reg.values(), (x_test_reg.shape[0], x_test_reg.shape[1]/len(use_features), len(use_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(x_train_reg.shape,y_train_reg.shape,x_test_reg.shape,y_test_reg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg_model = Sequential()\n",
    "reg_model.add(LSTM(10, input_shape=(x_train_reg.shape[1], x_train_reg.shape[-1])))\n",
    "reg_model.add(Dense(1))\n",
    "reg_model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "last_epoch_th = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg_model.fit(x_train_reg, y_train_reg, epochs=1000, batch_size=500, verbose=1, initial_epoch = last_epoch_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "reg_model.save('model/lstm_regression_HFT.hdf5', overwrite=True)  # creates a HDF5 file 'my_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "reg_model = load_model('model/lstm_regression_HFT.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# make predictions\n",
    "pred_train = reg_model.predict(x_train_reg, verbose=1)\n",
    "pred_test = reg_model.predict(x_test_reg, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,9))\n",
    "train_line_real = plt.plot(df_train.index[n_steps:], df_train[use_feature][n_steps:], color=\"g\", label=\"Real (Training)\")\n",
    "train_line_pred = plt.plot(df_train.index[n_steps:], pred_train[:, 0], color=\"r\", label=\"Predict (Training)\")\n",
    "plt.legend(loc=\"best\", fontsize=15)\n",
    "plt.title('Real vs Predict (Training)', fontsize=20)\n",
    "plt.xlabel('Time', fontsize=20)\n",
    "plt.ylabel('Data', fontsize=20)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(16,9))\n",
    "test_line_real = plt.plot(df_test.index[n_steps:], df_test[use_feature][n_steps:], color=\"c\", label=\"Real (Testing)\")\n",
    "test_line_pred = plt.plot(df_test.index[n_steps:], pred_test[:, 0], color=\"m\", label=\"Predict (Testing)\")\n",
    "plt.legend(loc=\"best\", fontsize=15)\n",
    "plt.title('Real vs Predict (Testing)', fontsize=20)\n",
    "plt.xlabel('Time', fontsize=20)\n",
    "plt.ylabel('Data', fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate root mean squared error\n",
    "MSE_train = mean_squared_error(df_train[use_feature][n_steps:], pred_train[:, 0])\n",
    "print(MSE_train)\n",
    "MSE_test = mean_squared_error(df_test[use_feature][n_steps:], pred_test[:, 0])\n",
    "print(MSE_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}